## Model architecture.
model: 
  ## Shape can be: [global, window-based, hybrid].
  shape: window-based
  ## If shape is [hybrid] define n_windows, otherwise leave it null.
  n_windows: null
  ## If shape is [global, window-based] define window_size, otherwise leave it null.
  window_size: 10000
  ## If [window-based] define window_cloning, otherwise leave it null.
  window_cloning: false
  ## Distribution can be: [Unknown, Gaussian, Multi-Bernoulli, Uniform].
  distribution: Unknown  
  ## Definition of Quantizer:  
  ## If distribution is [Uniform] define codebook_size.
  quantizer:
      codebook_size: 64
      win_independent: false
      multi_head:
          using: false
          features: 8
  ## If model is conditioned define number of classes.
  conditioning:
      using: false
      num_classes: 7
  ## If denoising mode is turned on.
  denoising:
      using: false
      missing: 0
  ## Architecture of Encoder:
  ## Layers must be numbered sequentially (0,1,2,...).
  ## Layers must have the following parameters: [size, dropout, normalize, activation].
  ## [size] defines the input shape tensor to the layer.
  ## The actual size of the param matrix is [size_{i}] x [size_{i+1}].
  ## The last layer has only one parameter: [size].
  encoder:
    layer0:
      size: 80000
      dropout: 0
      normalize: true
      activation: ReLU
    layer1:
      size: 1250
      dropout: 0
      normalize: true
      activation: Tanh
    layer2: 
      size: 512
  ## Architecture of Decoder:
  ## Layers must be numbered sequentially (0,1,2,...).
  ## Layers must have the following parameters: [size, dropout, normalize, activation].
  ## [size] defines the input shape tensor to the layer.
  ## The actual size of the param matrix is [size_{i}] x [size_{i+1}].
  ## The last layer has only one parameter: [size].
  decoder: 
    layer0:
      size: 512
      dropout: 0
      normalize: true
      activation: ReLU
    layer1:
      size: 1250
      dropout: 0
      normalize: true
      activation: Sigmoid
    layer2:
      size: 80000

## Hyperparameters for training.
hyperparams:
  epochs: 2000
  batch_size: 512
  loss:
      ## Variational beta.
      beta: 1,
      ## Varloss: using.
      varloss: false
  ## Optimizer algorithm can be: [Adam, AdamW].
  optimizer:
       algorithm: AdamW
       lr: 0.0001
       weight_decay: 0.01
  ## Scheduler can be: [null, plateau, step, multistep, exponential].
  scheduler: 
       method: plateau
       factor: 0.1
       patience: 5
       threshold: 0.0001
       mode: rel
  ## TR simulation can be: [offline, online].
  ## [offline] loads a dataset from disk.
  ## [online] performs simulation on-the-fly.
  ## [n_batches] determines the length of an epoch in terms of batches in [online] simulation.
  ## In [offline] simulation [n_batches, device, mode] are neglected.
  ## [mode] can be [uniform, exponential, pre-defined, fix].
  training:
       simulation: online
       n_batches: 5
       device: cpu
       mode: uniform
       balanced: true
  ## VD simulation can be: [offline].
  ## [offline] loads a dataset from disk.
  validation:
       simulation: offline
       ## Scheduler sets the rate on which validation is done. E.g.:
       ## 1: validation is done at the end of each epoch.
       ## 100: validation is done each 100 epochs.
       scheduler: 10
  ## TS simulation can be: [offline].
  ## [offline] loads a dataset from disk.
  testing:
      simulation: offline
      ## Scheduler sets the rate on which testing is done. E.g.:
      ## 1: testing is done at the end of each epoch.
      ## 100: testing is done each 100 epochs.
      scheduler: 150
  ## Checkpoint params.
  checkpointing:
      ## Scheduler sets the rate on which checkpointing is done. E.g.:
      ## 1: checkpointing is done at the end of each epoch.
      ## 100: checkpointing is done each 100 epochs.
      scheduler: 10
